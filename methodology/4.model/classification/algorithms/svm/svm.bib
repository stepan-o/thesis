@book{RaschkaMirjalili2017,
    address = {Birmingham, UK},
    author = {Raschka, Sebastian and Mirjalili, Vahid},
    edition = {2},
    isbn = {978-1787125933},
    keywords = {Clustering,Data Science,Deep Learning,
 Machine Learning,Neural Networks,Programming,
 Supervised Learning},
    publisher = {Packt Publishing},
    title = {{ Python Machine Learning, 2nd Ed. }},
    year = {2017}
}
@techreport{Fan2008,
    abstract = {LIBLINEAR is an open source library for large-scale linear classification. It supports logistic regression and linear support vector machines. We provide easy-to-use command-line tools and library calls for users and developers. Comprehensive documents are available for both beginners and advanced users. Experiments demonstrate that LIBLINEAR is very efficient on large sparse data sets.},
    author = {Fan, Rong-En and Chang, Kai-Wei and Hsieh, Cho-Jui and Wang, Xiang-Rui and Lin, Chih-Jen},
    booktitle = {Journal of Machine Learning Research},
    file = {:C$\backslash$:/Users/HP USER/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Fan et al. - 2008 - LIBLINEAR A Library for Large Linear Classification.pdf:pdf},
    keywords = {large-scale linear classification,logistic regression,machine learning,open source,support vector machines},
    pages = {1871--1874},
    title = {{ LIBLINEAR: A Library for Large Linear Classification }},
    url = {http://www.csie.ntu.edu.tw/},
    volume = {9},
    year = {2008}
}
@book{Vapnik2000,
    abstract = {The aim of this book is to discuss the fundamental ideas which lie behind the statistical theory of learning and generalization. It considers learning as a general problem of function estimation based on empirical data. Omitting proofs and technical details, the author concentrates on discussing the main results of learning theory and their connections to fundamental problems in statistics. These include: * the setting of learning problems based on the model of minimizing the risk functional from empirical data * a comprehensive analysis of the empirical risk minimization principle including necessary and sufficient conditions for its consistency * non-asymptotic bounds for the risk achieved using the empirical risk minimization principle * principles for controlling the generalization ability of learning machines using small sample sizes based on these bounds * the Support Vector methods that control the generalization ability when estimating function using small sample size. The second edition of the book contains three new chapters devoted to further development of the learning theory and SVM techniques. These include: * the theory of direct method of learning based on solving multidimensional integral equations for density, conditional probability, and conditional density estimation * a new inductive principle of learning. Written in a readable and concise style, the book is intended for statisticians, mathematicians, physicists, and computer scientists. Vladimir N. Vapnik is Technology Leader AT { \& } T Labs-Research and Professor of London University.},
    address = {New York, NY},
    author = {Vapnik, Vladimir N.},
    doi = {https://doi.org/10.1007/978-1-4757-3264-1},
    edition = {Second edi},
    file = {:C$\backslash$:/Users/HP USER/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Vapnik - 2000 - The Nature of Statistical Learning Theory.pdf:pdf},
    isbn = {978-1-4757-3264-1},
    publisher = {Springer, New York, NY},
    title = {{ The Nature of Statistical Learning Theory }},
    url = {https://link.springer.com/book/10.1007/978-1-4757-3264-1 { \# } about},
    year = {2000}
}
@techreport{Hsu2003,
    abstract = {The support vector machine (SVM) is a popular classification technique. However, beginners who are not familiar with SVM often get unsatisfactory results since they miss some easy but significant steps. In this guide, we propose a simple procedure which usually gives reasonable results.},
    address = {Taipei},
    author = {Hsu, Chih-Wei and Chang, Chih-Chung and Lin, Chih-Jen},
    file = {:C$\backslash$:/Users/HP USER/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Hsu, Chang, Lin - 2003 - A Practical Guide to Support Vector Classification.pdf:pdf},
    institution = {National Taiwan University},
    title = {{ A Practical Guide to Support Vector Classification }},
    url = {http://www.csie.ntu.edu.tw/ { ~ } cjlin},
    year = {2016}
}
@techreport{Chang2001,
    abstract = {LIBSVM is a library for Support Vector Machines (SVMs). We have been actively developing this package since the year 2000. The goal is to help users to easily apply SVM to their applications. LIBSVM has gained wide popularity in machine learning and many other areas. In this article, we present all implementation details of LIBSVM. Issues such as solving SVM optimization problems, theoretical convergence, multi-class classification, probability estimates, and parameter selection are discussed in detail.},
    address = {Taipei},
    author = {Chang, Chih-Chung and Lin, Chih-Jen},
    file = {:C$\backslash$:/Users/HP USER/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Chang, Lin - 2001 - LIBSVM A Library for Support Vector Machines.pdf:pdf},
    institution = {National Taiwan University},
    keywords = {Classification,LIBSVM,SVM,optimization,regression,support vector ma-chines},
    title = {{ LIBSVM: A Library for Support Vector Machines }},
    url = {www.csie.ntu.edu.tw/},
    year = {2001}
}
@techreport{Burges1998,
    abstract = {The tutorial starts with an overview of the concepts of VC dimension and structural risk minimization. We then describe linear Support Vector Machines (SVMs) for separable and non-separable data, working through a non-trivial example in detail. We describe a mechanical analogy, and discuss when SVM solutions are unique and when they are global. We describe how support vector training can be practically implemented, and discuss in detail the kernel mapping technique which is used to construct SVM solutions which are nonlinear in the data. We show how Support Vector machines can have very large (even infinite) VC dimension by computing the VC dimension for homogeneous polynomial and Gaussian radial basis function kernels. While very high VC dimension would normally bode ill for generalization performance, and while at present there exists no theory which shows that good generalization performance is guaranteed for SVMs, there are several arguments which support the observed high accuracy of SVMs, which we review. Results of some experiments which were inspired by these arguments are also presented. We give numerous examples and proofs of most of the key theorems. There is new material, and I hope that the reader will find that even old material is cast in a fresh light.},
    address = {Boston, MA},
    author = {Burges, Christopher J C},
    file = {:C$\backslash$:/Users/HP USER/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Burges - Unknown - A Tutorial on Support Vector Machines for Pattern Recognition.pdf:pdf},
    institution = {Bell Laboratories, Lucent Technologies},
    keywords = {Pattern Recognition,Statistical Learning Theory,Support Vector Machines,VC Dimension},
    pages = {121--167},
    title = {{ A Tutorial on Support Vector Machines for Pattern Recognition }},
    url = {https://www.di.ens.fr/ { ~ } mallat/papiers/svmtutorial.pdf},
    year = {1998}
}
