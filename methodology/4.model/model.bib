@misc{Brownlee2013,
abstract = {Machine learning algorithms learn from data. It is critical that you feed them the right data for the problem you want to solve. Even if you have good data, you need to make sure that it is in a useful scale, format and even that meaningful features are included. In this post you will learn how to prepare data for a machine learning algorithm. This is a big topic and you will cover the essentials.},
author = {Brownlee, Jason},
booktitle = {machinelearningmastery.com},
keywords = {ML,OSEMN,clean,data science,scrub},
mendeley-tags = {ML,OSEMN,clean,data science,scrub},
title = {{How to Prepare Data For Machine Learning}},
url = {https://machinelearningmastery.com/how-to-prepare-data-for-machine-learning/},
year = {2013}
}
@article{Wolpert1996,
abstract = {This is the first of two papers that use off-training set {\{}(OTS){\}} error to investigate the assumption-free relationship between learning algorithms. This first paper discusses the senses in which there are no {\{}$\backslash$textbackslash{\}}textita priori distinctions between learning algorithms. {\{}(The{\}} second paper discusses the senses in which there are such distinctions.) In this first paper it is shown, loosely speaking, that for any two algorithms A and B, there are “as many” targets (or priors over targets) for which A has lower expected {\{}OTS{\}} error than B as vice-versa, for loss functions like zero-one loss. In particular, this is true if A is cross-validation and B is “anti-cross-validation” (choose the learning algorithm with largest cross-validation error). This paper ends with a discussion of the implications of these results for computational learning theory. It is shown that one can not say: if empirical misclassification rate is low; the {\{}Vapnik-Chervonenkis{\}} dimension of your generalizer is small; and the training set is large, then with high probability your {\{}OTS{\}} error is small. Other implications for “membership queries” algorithms and “punting” algorithms are also discussed.},
author = {Wolpert, David H},
file = {:C$\backslash$:/Users/HP USER/Documents/Thesis/Data science papers/Wolpert(1996) - The Lack of A Priori Distinctions Between Learning Algorithms.pdf:pdf},
journal = {Neural Computation},
number = {7},
pages = {1391--1420},
title = {{The Lack of A Priori Distinctions Between Learning Algorithms}},
volume = {8},
year = {1996}
}
@book{RaschkaMirjalili2017,
address = {Birmingham, UK},
author = {Raschka, Sebastian and Mirjalili, Vahid},
edition = {2},
isbn = {978-1787125933},
keywords = {Clustering,Data Science,Deep Learning,Machine Learning,Neural Networks,Programming,Supervised Learning},
publisher = {Packt Publishing},
title = {{ Python Machine Learning, 2nd Ed. }},
year = {2017}
}
