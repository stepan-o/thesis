@misc{Brownlee2013,
abstract = {Machine learning algorithms learn from data. It is critical that you feed them the right data for the problem you want to solve. Even if you have good data, you need to make sure that it is in a useful scale, format and even that meaningful features are included. In this post you will learn how to prepare data for a machine learning algorithm. This is a big topic and you will cover the essentials.},
author = {Brownlee, Jason},
booktitle = {machinelearningmastery.com},
keywords = {ML,OSEMN,clean,data science,scrub},
mendeley-tags = {ML,OSEMN,clean,data science,scrub},
title = {{How to Prepare Data For Machine Learning}},
url = {https://machinelearningmastery.com/how-to-prepare-data-for-machine-learning/},
year = {2013}
}
@techreport{Wolpert1997,
abstract = {A framework is developed to explore the connection between effective optimization algorithms and the problems they are solving. A number of "no free lunch" (NFL) theorems are presented which establish that for any algorithm, any elevated performance over one class of problems is offset by performance over another class. These theorems result in a geometric interpretation of what it means for an algorithm to be well suited to an optimization problem. Applications of the NFL theorems to information-theoretic aspects of optimization and benchmark measures of performance are also presented. Other issues addressed include time-varying optimization problems and a priori "head-to-head" minimax distinctions between optimization algorithms, distinctions that result despite the NFL theorems' enforcing of a type of uniformity over all algorithms.},
author = {Wolpert, David H and Macready, William G},
booktitle = {IEEE TRANSACTIONS ON EVOLUTIONARY COMPUTATION},
file = {:C$\backslash$:/Users/HP USER/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Wolpert, Macready - 1997 - No Free Lunch Theorems for Optimization.pdf:pdf},
keywords = {Index Terms-Evolutionary algorithms,information theory,optimization},
number = {1},
pages = {67},
title = {{No Free Lunch Theorems for Optimization}},
url = {https://ti.arc.nasa.gov/m/profile/dhw/papers/78.pdf},
volume = {1},
year = {1997}
}
@book{RaschkaMirjalili2017,
address = {Birmingham, UK},
author = {Raschka, Sebastian and Mirjalili, Vahid},
edition = {2},
isbn = {978-1787125933},
keywords = {Clustering,Data Science,Deep Learning,Machine Learning,Neural Networks,Programming,Supervised Learning},
publisher = {Packt Publishing},
title = {{ Python Machine Learning, 2nd Ed. }},
year = {2017}
}
@article{Ferri1994,
abstract = {The combinatorial search problem arising in feature selection in high dimensional spaces is considered. Recently developed techniques based on the classical sequential methods and the (l, r) search called Floating search algorithms are compared against the Genetic approach to feature subset search. Both approaches have been designed with the view to give a good compromise between efficiency and effectiveness for large problems. The purpose of this paper is to investigate the applicability of these techniques to high dimensional problems of feature selection. The aim is to establish whether the properties inferred for these techniques from medium scale experiments involving up to a few tens of dimensions extend to dimensionalities of one order of magnitude higher. Further, relative merits of these techniques vis-a-vis such high dimensional problems are explored and the possibility of exploiting the best aspects of these methods to create a composite feature selection procedure with superior properties is considered. {\textcopyright} 1994, Elsevier Science {\&} Technology. All rights reserved.},
author = {Ferri, F. J. and Pudil, P. and Hatef, M. and Kittler, J.},
doi = {10.1016/B978-0-444-81892-8.50040-7},
file = {:C$\backslash$:/Users/HP USER/Documents/repos/thesis/methodology/references/Ferri (1994) Comparative Study of Techniques for Large-Scale Feature Selection.pdf:pdf},
issn = {09230459},
journal = {Machine Intelligence and Pattern Recognition},
number = {C},
pages = {403--413},
title = {{Comparative study of techniques for large-scale feature selection}},
volume = {16},
year = {1994}
}
