@article{Anscombe1973,
abstract = {Graphs are essential to good statistical analysis. Ordinary scatterplots are discussed in relation to regression analysis.},
archivePrefix = {arXiv},
arxivId = {arXiv:1011.1669v3},
author = {Anscombe, F. J.},
doi = {10.1080/00031305.1973.10478966},
eprint = {arXiv:1011.1669v3},
file = {:C$\backslash$:/Users/HP USER/Documents/Thesis/Data science papers/Anscombe(1973) - Graphs in statistical analysis.pdf:pdf},
isbn = {00031305},
issn = {0003-1305},
journal = {The American Statistician},
number = {1},
pages = {17--21},
pmid = {2577},
title = {{Graphs in Statistical Analysis}},
url = {http://www.sjsu.edu/faculty/gerstman/StatPrimer/anscombe1973.pdf},
volume = {27},
year = {1973}
}
@article{Arribas-Bel2014,
abstract = {In this paper, I review the recent emergence of three groups of data sources and assess some of the opportunities and challenges they pose for the understanding of cities, particularly in the context of the Regional Science and urban research agenda. These are data collected from mobile sensors carried by individuals, data derived from businesses moving their activity online and government data released in an open format. Although very different from each other, they are all becoming available as a side-effect since they were created with different purposes but their degree of popularity, pervasiveness and ease of access is turning them into interesting alternatives for researchers. Existing projects and initiatives that conform to each class are featured as illustrative examples of these new potential sources of knowledge. {\textcopyright} 2013 Elsevier Ltd.},
author = {Arribas-Bel, Daniel},
doi = {10.1016/j.apgeog.2013.09.012},
file = {:C$\backslash$:/Users/HP USER/Documents/Thesis/Data science papers/D.Arribas-Bell - Accidental, open and everywhere - Emerging data sources for the.pdf:pdf},
isbn = {0143-6228},
issn = {01436228},
journal = {Applied Geography},
keywords = {Cities,Data sources,Open data},
pages = {45--53},
publisher = {Elsevier Ltd},
title = {{Accidental, open and everywhere: Emerging data sources for the understanding of cities}},
url = {http://dx.doi.org/10.1016/j.apgeog.2013.09.012},
volume = {49},
year = {2014}
}
@techreport{Ashby2018,
abstract = {The purpose of this report is to summarize the Transportation Tomorrow Survey data at the Ward level for the City of Toronto. The summary is presented in tabular format at different levels of detail, namely the Greater Toronto and Hamilton Area, the City of Toronto, and the wards within the City of Toronto. In total, there are 44 wards within the City of Toronto. The information presented includes socio-demographic and travel characteristics. In addition to presenting the magnitude of the trips coming into and leaving an area, the summary tables also describe travel characteristics such as travel purpose, trip start time, travel distance and travel mode choice.},
address = {Toronto},
author = {Ashby, Bess},
file = {:C$\backslash$:/Users/HP USER/Documents/repos/thesis/methodology/references/TTS (2016) TTS Summaries By Toronto Wards.pdf:pdf},
institution = {malatest},
title = {{TTS 2016 City of Toronto Summary by Ward}},
url = {http://dmg.utoronto.ca/pdf/tts/2016/2016TTS{\_}Summaries{\_}Toronto{\_}Wards.pdf},
year = {2018}
}
@misc{BankofCanada2019,
abstract = {The Inflation Calculator uses monthly consumer price index (CPI) data from 1914 to the present to show changes in the cost of a fixed "basket" of consumer purchases. These include food, shelter, furniture, clothing, transportation, and recreation. An increase in this cost is called inflation. The calculator's results are based on the most recent month for which the CPI data are available. This will normally be about two months prior to the current month.},
author = {{Bank of Canada}},
booktitle = {https://www.bankofcanada.ca/},
title = {{Inflation Calculator}},
url = {https://www.bankofcanada.ca/rates/related/inflation-calculator/},
year = {2019}
}
@unpublished{Batty2008,
abstract = {Cities have been treated as systems for fifty year but only in the last two decades has the focus changed from aggregate equilibrium systems to more evolving systems whose structure merges from the bottom up. We first outline the rudiments of the traditional approach focusing on equilibrium and then discuss how the paradigm has changed to one which treats cities as emergent phenomena generated through a combination of hierarchical levels of decision, driven in decentralized fashion. This is consistent with the complexity sciences which dominate the simulation of urban form and function. We begin however with a review of equilibrium models, particularly those based on spatial interaction, and we then explore how simple dynamic frameworks can be fashioned to generate more realistic models. In exploring dynamics, nonlinear systems which admit chaos and bifurcation have relevance but recently more pragmatic schemes of structuring urban models based on cellular automata and agent-based modeling principles have come to the fore. Most urban models deal with the city in terms of the location of its economic and demographic activities but there is also a move to link such models to urban morphologies which are clearly fractal in structure. Throughout this chapter, we show how key concepts in complexity such as scaling, self-similarity and far-from-equilibrium structures dominate our current treatment of cities, how we might simulate their functioning and how we might predict their futures. We conclude with the key problems that dominate the field and suggest how these might be tackled in future research. †in},
address = {London, UK},
archivePrefix = {arXiv},
arxivId = {0706.0024},
author = {Batty, Michael},
booktitle = {Analysis},
doi = {10.1103/PhysRevE.78.016110},
eprint = {0706.0024},
file = {:C$\backslash$:/Users/HP USER/Documents/repos/thesis/methodology/references/Batty (2008) Cities as complex systems.pdf:pdf},
institution = {UCL Centre for Advanced Spatial Analysis, University College London},
isbn = {9780749215453},
issn = {14671298},
number = {0},
pages = {0--18},
pmid = {18764023},
title = {{Cities as Complex Systems: Scaling, Interactions, Networks, Dynamics and Urban Morphologies}},
url = {http://www.springerlink.com/index/j3863x4mm7gu8645.pdf},
volume = {44},
year = {2008}
}
@techreport{Bellman1954,
abstract = {This paper is the text of an invited address before the annual summer meeting of the American Mathematical Society at Laramie, Wyoming, September 2, 1954.},
author = {Bellman, Richard},
file = {:C$\backslash$:/Users/HP USER/Documents/repos/thesis/methodology/references/Bellman (1954) The Theory of Dynamic Programming.pdf:pdf},
institution = {The RAND Corporation},
pages = {1--23},
title = {{The Theory of Dynamic Programming}},
url = {http://smo.sogang.ac.kr/doc/bellman.pdf},
year = {1954}
}
@incollection{Ben-Gal2005,
abstract = {Outlier detection is a primary step in many data-mining applications. We present several methods for outlier detection, while distinguishing between univariate vs. multivariate techniques and parametric vs. nonparametric procedures. In presence of outliers, special attention should be taken to assure the robustness of the used estimators. Outlier detection for data mining is often based on distance measures, clustering and spatial methods.},
address = {Tel-Aviv},
author = {Ben-Gal, Irad},
booktitle = {Data Mining and Knowledge Discovery Handbook: A Complete Guide for Practitioners and Researchers},
chapter = {Chapter 1},
file = {:C$\backslash$:/Users/HP USER/Documents/repos/thesis/methodology/references/Ben-Gal (2005) Outlier detection.pdf:pdf},
isbn = {0-387-24435-2},
keywords = {Network structure,Pedestrian movement,Spatial planning,Transport networks,Urban design},
pages = {1--16},
publisher = {Kluwer Academic Publishers},
title = {{Outlier Detection}},
year = {2005}
}
@article{Bettencourt2013,
abstract = {Despite the increasing importance of cities in human societies, our ability to understand them scientifically and manage them in practice has remained limited. The greatest difficulties to any scientific approach to cities have resulted from their many interdependent facets, as social, economic, infrastructural, and spatial complex systems that exist in similar but changing forms over a huge range of scales. Here, I show how all cities may evolve according to a small set of basic principles that operate locally. A theoretical framework was developed to predict the average social, spatial, and infrastructural properties of cities as a set of scaling relations that apply to all urban systems. Confirmation of these predictions was observed for thousands of cities worldwide, from many urban systems at different levels of development. Measures of urban efficiency, capturing the balance between socioeconomic outputs and infrastructural costs, were shown to be independent of city size and might be a useful means to evaluate urban planning strategies.},
author = {Bettencourt, Lu{\'{i}}s M.A.},
doi = {10.1126/science.1235823},
file = {:C$\backslash$:/Users/HP USER/Documents/repos/thesis/methodology/references/Bettencourt (2013) The Origins of Scaling in Cities.pdf:pdf},
issn = {10959203},
journal = {Science},
number = {6139},
pages = {1438--1441},
title = {{The origins of scaling in cities}},
volume = {340},
year = {2013}
}
@article{Bouchard1965,
abstract = {This research provides evaluations of the gravity model as an analytical tool for simulating present and forecasting future urban trip distribution patterns. The evaluations were made by comparing gravity model trip interchanges with those found in home interview origin and destination surveys conducted in Washington, D. C., in 1948 and 1955. The 1955 survey data were used for calibrating the basic gravity model and for testing this model for its ability to simulate current travel patterns. The 1948 survey provided comprehensive data to analyze the forecasts made by the calibrated model. The gr avity model will give satisfactory results if properly calibrated and tested. The level of accuracy obtained by fore- casting trip distribution patterns in 1948 was comparable to the level of model accuracy for the base year.},
author = {Bouchard, Richard J and Pyers, Clyde E},
file = {:C$\backslash$:/Users/HP USER/Documents/repos/thesis/methodology/references/Bouchard (1965) Use of Gravity Model for Describing Urban Travel.pdf:pdf},
journal = {Highway Research Record},
number = {88},
pages = {1--43},
title = {{Use of gravity model for describing urban travel: An analysis and critique}},
year = {1965}
}
@book{Bourne1982,
author = {Bourne, Larry S},
isbn = {9780195030327},
publisher = {Oxford University Press},
title = {{Internal structure of the city: readings on urban form, growth, and policy}},
url = {https://books.google.nl/books?id=RMtYAAAAMAAJ},
year = {1982}
}
@book{Brett2009,
abstract = {Offering the tools needed to evaluate trends and understand key factors affecting the real estate market, this book explains how to get started, where to get information, and how to apply the basic techniques to a variety of development types. This practical primer offers a step-by-step approach to developing property—whether public or private sector—and shows how market-analysis methods have been employed in real projects. The 13 case studies written by top market analysts provide models that can be applied to multifamily, hotel, office, industrial, entertainment, mixed-use, and/or master-planned communities. This is an excellent reference for real estate students and professionals to maximize potential in a troubled market.},
address = {Toronto},
author = {Brett, Deborah and Schmitz, Adrienne},
edition = {Second Edi},
isbn = {9780874203653},
publisher = {Urban Land Institute},
title = {{Real Estate Market Analysis: Methods and Case Studies, Second Edition}},
url = {https://uli.bookstore.ipgbook.com/real-estate-market-analysis-products-9780874203653.php},
year = {2009}
}
@misc{Brownlee2013,
abstract = {Machine learning algorithms learn from data. It is critical that you feed them the right data for the problem you want to solve. Even if you have good data, you need to make sure that it is in a useful scale, format and even that meaningful features are included. In this post you will learn how to prepare data for a machine learning algorithm. This is a big topic and you will cover the essentials.},
author = {Brownlee, Jason},
booktitle = {machinelearningmastery.com},
keywords = {ML,OSEMN,clean,data science,scrub},
mendeley-tags = {ML,OSEMN,clean,data science,scrub},
title = {{How to Prepare Data For Machine Learning}},
url = {https://machinelearningmastery.com/how-to-prepare-data-for-machine-learning/},
year = {2013}
}
@article{Chen2016,
abstract = {The last decade has witnessed very active development in two broad, but separate fields, both involving understanding and modeling of how individuals move in time and space (hereafter called "travel behavior analysis" or "human mobility analysis"). One field comprises transportation researchers who have been working in the field for decades and the other involves new comers from a wide range of disciplines, but primarily computer scientists and physicists. Researchers in these two fields work with different datasets, apply different methodologies, and answer different but overlapping questions. It is our view that there is much, hidden synergy between the two fields that needs to be brought out. It is thus the purpose of this paper to introduce datasets, concepts, knowledge and methods used in these two fields, and most importantly raise cross-discipline ideas for conversations and collaborations between the two. It is our hope that this paper will stimulate many future cross-cutting studies that involve researchers from both fields.},
author = {Chen, Cynthia and Ma, Jingtao and Susilo, Yusak and Liu, Yu and Wang, Menglin},
doi = {10.1016/j.trc.2016.04.005},
file = {:C$\backslash$:/Users/HP USER/Documents/repos/thesis/methodology/references/Chen (2016) Promises of Data for Travel Behavior.pdf:pdf},
issn = {0968090X},
journal = {Transportation Research Part C: Emerging Technologies},
keywords = {Big data,Human mobility,Small data,Transportation planning,Travel behavior},
pages = {285--299},
publisher = {Elsevier Ltd},
title = {{The promises of big data and small data for travel behavior (aka human mobility) analysis}},
url = {http://dx.doi.org/10.1016/j.trc.2016.04.005},
volume = {68},
year = {2016}
}
@book{Codd1990,
address = {Boston, MA},
author = {Codd, Edgar F},
file = {:C$\backslash$:/Users/HP USER/Documents/Thesis/Data science papers/codd-relational{\_}model{\_}db{\_}management.pdf:pdf},
isbn = {ISBN:0-201-14192-2},
publisher = {Addison-Wesley Longman Publishing Co.},
title = {{The relational model for database management : version 2}},
year = {1990}
}
@article{Crowley2017,
abstract = {{\textcopyright} 2017, Springer Science+Business Media, LLC. There is, in the twenty-first century, an intense interest in the nature of wicked problems and the complex tasks of identifying their scope, viable responses, and appropriate mechanisms and pathways towards achieving improvement. This preoccupation is timeless, but the discussion over several decades has benefited from Rittel and Webber's (Policy Sci 4(2):155–169, 1973) path breaking conceptualisation of wicked problems and the political argumentation needed to resolve them. This review revisits Rittel and Webber's work and its enduring significance, reflecting upon its broad uptake and impact in the policy sciences, an impact that continues to grow over time. We revisit how the classic 1973 paper came to be published in Policy Sciences, its innovative depiction of social problems, its rejection of rationalistic design, its acknowledgement of the subjectivities involved in problem identification and resolutions, and the consequent need for argumentative-based solution processes. We find great resonance in the paper with contemporary problem solving preoccupations, not least that the political context is crucial, that argumentation must be transparent and robust, and that policy interventions may have consequences that cannot be easily controlled in open and highly pluralised social systems.},
author = {Crowley, Kate and Head, Brian W.},
doi = {10.1007/s11077-017-9302-4},
file = {:C$\backslash$:/Users/HP USER/Documents/repos/thesis/methodology/references/The enduring challenge of wicked problems, revisiting Rittel and Webber.pdf:pdf},
issn = {15730891},
journal = {Policy Sciences},
keywords = {Horst Rittel,Melvin Webber,Policy solutions,Systems theory,Wicked problems},
number = {4},
pages = {539--547},
publisher = {Springer US},
title = {{The enduring challenge of ‘wicked problems': revisiting Rittel and Webber}},
volume = {50},
year = {2017}
}
@misc{DataManagementGroup2014,
abstract = {Founded in 1988, the Data Management Group is the custodian of the data sets derived from the Transportation Tomorrow Surveys (TTS) on urban travel conducted every five years since 1986. The 1986 and 1991 data files contain detailed information on over 500,000 trips in the Greater Toronto and Hamilton area. The survey area has expanded through the years and the latest survey, 2016 TTS, contains over 790,000 trips in South Central Ontario from Waterloo Region to Peterborough County. These data sets form the factual basis for virtually every transportation planning study carried out in the area by and for local, regional and provincial agencies. In addition, the data sets form the empirical base for a wide variety of research projects.},
author = {{Data Management Group}},
booktitle = {http://dmg.utoronto.ca},
title = {{Data Management Group at the University of Toronto Transportation Research Institute}},
url = {http://dmg.utoronto.ca},
year = {2014}
}
@misc{DataManagementGroup2019,
abstract = {The Transportation Tomorrow Survey (TTS) and Metropolitan Toronto and Region Transportation Study (MTARTS) are origin destination travel surveys. The Survey Data is available through the Internet Data Retrieval System (iDRS). This page contains the geographic data files for the spatial aggregations used in iDRS. The finest level of spatial aggregation available through iDRS is that of the Traffic Zone. The Traffic Zone is a polygon which typically falls along the centre line of roads or the natural geographic boundaries. The survey area is subdivided into traffic zones based on the planning needs of the participating agencies.},
author = {{Data Management Group}},
booktitle = {http://dmg.utoronto.ca},
title = {{Survey Boundary Files}},
url = {http://dmg.utoronto.ca/survey-boundary-files},
year = {2019}
}
@article{Dietterich1995,
abstract = {The term "bias" is widely used--and with different meanings--in the fields of machine learning and statistics. This paper clarifies the uses of this term and shows how to measure and visualize the statistical bias and variance of learning algorithms. Statistical bias and variance can be applied to diagnose problems with machine learning bias, and the paper shows four examples of this. Finally, the paper discusses methods of reducing bias and variance. Methods based on voting can reduce variance, and the paper compares Breiman's bagging method and our own tree randomization method for voting decision trees. Both methods uniformly improve performance on data sets from the Irvine repository. Tree randomization yields perfect performance on the Letter Recognition task. A weighted nearest neighbor algorithm based on the infinite bootstrap is also introduced. In general, decision tree algorithms have moderate-to-high variance, so an important implication of this work is that variance--rat...},
author = {Dietterich, Thomas G and Kong, Eun Bae},
file = {:C$\backslash$:/Users/HP USER/Documents/repos/thesis/methodology/references/Dietterich (1995) Machine Learning Bias, Statistical Bias, and Statistical Variance of Decision Tree Algorithms.pdf:pdf},
pages = {0--13},
title = {{Machine Learning Bias, Statistical Bias, and Statistical Variance of Decision Tree Algorithms}},
year = {1995}
}
@misc{DMTISpatialInc.2014,
abstract = {DMTI has been providing industry leading enterprise Location Intelligence solutions for more than a decade to Global 2000 companies and government agencies. DMTI's world-class Location Hub platform uniquely identifies, validates and maintains a universe of location-based data. DMTI is the creator of market leading Mapping Solutions and maintains the gold standard for GIS location-based data in Canada.},
address = {Markham, Ontario},
author = {{DMTI Spatial Inc.}},
file = {:C$\backslash$:/Users/HP USER/Documents/repos/thesis/methodology/references/DMTI (2014) CanMapRouteLogistics Manual.pdf:pdf},
number = {2014.2},
pages = {1--130},
publisher = {DMTI Spatial Inc.},
title = {{CanMap {\textregistered} RouteLogistics User Manual v2014.2}},
url = {www.dmtispatial.com A},
year = {2014}
}
@article{Fayyad1996a,
abstract = {Data mining and knowledge discovery in databases have been attracting a significant amount of research, industry, and media attention
of late. What is all the excitement about? This article provides an overview of this emerging
field, clarifying how data mining and knowledge discovery in databases are related both to each
other and to related fields, such as machine learning, statistics, and databases. The article
mentions particular real-world applications, specific data-mining techniques, challenges involved
in real-world applications of knowledge discovery, and current and future research directions
in the field.},
author = {Fayyad, Usama and Piatetsky-Shapiro, Gregory and Smyth, Padhraic},
file = {:C$\backslash$:/Users/HP USER/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Fayyad, Piatetsky-Shapiro, Smyth - 1996 - From Data Mining to Knowledge Discovery in Databases.pdf:pdf},
journal = { American Association for Artificial Intelligence},
pages = {37--54},
title = {{From Data Mining to Knowledge Discovery in Databases}},
url = {https://www.kdnuggets.com/gpspubs/aimag-kdd-overview-1996-Fayyad.pdf},
year = {1996}
}
@article{Ferri1994,
abstract = {The combinatorial search problem arising in feature selection in high dimensional spaces is considered. Recently developed techniques based on the classical sequential methods and the (l, r) search called Floating search algorithms are compared against the Genetic approach to feature subset search. Both approaches have been designed with the view to give a good compromise between efficiency and effectiveness for large problems. The purpose of this paper is to investigate the applicability of these techniques to high dimensional problems of feature selection. The aim is to establish whether the properties inferred for these techniques from medium scale experiments involving up to a few tens of dimensions extend to dimensionalities of one order of magnitude higher. Further, relative merits of these techniques vis-a-vis such high dimensional problems are explored and the possibility of exploiting the best aspects of these methods to create a composite feature selection procedure with superior properties is considered. {\textcopyright} 1994, Elsevier Science {\&} Technology. All rights reserved.},
author = {Ferri, F. J. and Pudil, P. and Hatef, M. and Kittler, J.},
doi = {10.1016/B978-0-444-81892-8.50040-7},
file = {:C$\backslash$:/Users/HP USER/Documents/repos/thesis/methodology/references/Ferri (1994) Comparative Study of Techniques for Large-Scale Feature Selection.pdf:pdf},
issn = {09230459},
journal = {Machine Intelligence and Pattern Recognition},
number = {C},
pages = {403--413},
title = {{Comparative study of techniques for large-scale feature selection}},
volume = {16},
year = {1994}
}
@book{Hawkins1980,
abstract = {The problem of outliers is one of the oldest in statistics, and during the last century and a half interest in it has waxed and waned several times. Currently it is once again an active research area after some years of relative neglect, and recent work has solved a number of old problems in outlier theory, and identified new ones. The major results are, however, scattered amongst many journal articles, and for some time there has been a clear need to bring them together in one place. That was the original intention of this monograph: but during execution it became clear that the existing theory of outliers was deficient in several areas, and so the monograph also contains a number of new results and conjectures. In view of the enormous volume of literature on the outlier problem and its cousins, no attempt has been made to make the coverage exhaustive. The material is concerned almost entirely with the use of outlier tests that are known (or may reasonably be expected) to be optimal in some way. Such topics as robust estimation are largely ignored, being covered more adequately in other sources. The numerous ad hoc statistics proposed in the early work on the grounds of intuitive appeal or computational simplicity also are not discussed in any detail.},
address = {London, UK},
author = {Hawkins, Douglas M},
isbn = {041221900X},
pages = {1--188},
publisher = {Chapman and Hall},
title = {{Identification of outliers}},
year = {1980}
}
@article{Hunt2005,
abstract = {Various alternative frameworks are available for modelling urban land-use transport interaction. This paper provides a detailed review of six of these frameworks that have been or are currently being used to develop operational models. The intention is to indicate what is the general nature of the current state of practice and what is now available for practical modelling work in the area. The intention is also to compare the current state of practice with what might be the ideal in various respects. The six frameworks reviewed (ITLUP, MEPLAN, TRANUS, MUSSA, NYMTC-LUM and UrbanSim) are considered in terms of their representations of physical systems, decision-makers and processes, along with various more general modelling and implementation issues. None matches the ideal as envisaged here in all respects. However, a wide range of policy considerations can be handled explicitly with what is available, and more recent developments show an encouraging trend towards expansion in the scope of what can be considered. Further strengthening of the behavioural basis and relaxation of some of the more restrictive assumptions would appear to be both appropriate and likely in the future.},
author = {Hunt, J. D. and Kriger, D. S. and Miller, E. J.},
doi = {10.1080/0144164052000336470},
file = {:C$\backslash$:/Users/HP USER/Documents/repos/thesis/methodology/references/Hunt (2005) Current Operational Urban Land-use-Transport Modelling Frameworks - A review.pdf:pdf},
issn = {01441647},
journal = {Transport Reviews},
number = {3},
pages = {329--376},
title = {{Current operational urban land-use-transport modelling frameworks: A review}},
volume = {25},
year = {2005}
}
@article{Iacono2008,
abstract = {Modern urban regions are highly complex entities. Despite the difficulty of modeling every relevant aspect of an urban region, researchers have produced a rich variety of models dealing with interrelated processes of urban change. The most popular types of models have been those dealing with the relationship between transportation network growth and changes in land use and the location of economic activity, embodied in the concept of accessibility. This article reviews some of the more common frameworks for modeling transportation and land use change, illustrating each with some examples of operational models that have been applied to real-world settings. It then identifies new directions for future research in urban modeling and notes the important contributions of the field to date.},
author = {Iacono, Michael and Levinson, David and El-Geneidy, Ahmed},
doi = {10.1177/0885412207314010},
file = {:C$\backslash$:/Users/HP USER/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Iacono, Levinson, El-Geneidy - 2008 - Models of transportation and land use change A guide to the territory.pdf:pdf},
isbn = {08854122 (ISSN)},
issn = {08854122},
journal = {Journal of Planning Literature},
keywords = {Gravity model,Land use,Mathematical models,Microsimulation,Transportation planning,Urban growth},
pmid = {255408600001},
title = {{Models of transportation and land use change: A guide to the territory}},
year = {2008}
}
@incollection{Jacobs1961,
abstract = {A direct and fundamentally optimistic indictment of the short-sightedness and intellectual arrogance that has characterized much of urban planning in this century, The Death and Life of Great American Cities has, since its first publication in 1961, become the standard against which all endeavors in that field are measured. In prose of outstanding immediacy, Jane Jacobs writes about what makes streets safe or unsafe; about what constitutes a neighborhood, and what function it serves within the larger organism of the city; about why some neighborhoods remain impoverished while others regenerate themselves. She writes about the salutary role of funeral parlors and tenement windows, the dangers of too much development money and too little diversity. Compassionate, bracingly indignant, and always keenly detailed, Jane Jacobs's monumental work provides an essential framework for assessing the vitality of all cities.},
archivePrefix = {arXiv},
arxivId = {arXiv:gr-qc/9809069v1},
author = {Jacobs, Jane},
booktitle = {New York},
doi = {10.2307/794509},
eprint = {9809069v1},
isbn = {067974195X},
issn = {00027294},
pages = {Alexander, C., Ishikawa, S., {\&} Silverstein, M. (19},
pmid = {1622650174},
primaryClass = {arXiv:gr-qc},
title = {{The Death and Life of Great American Cities}},
volume = {71},
year = {1961}
}
@book{James2013,
abstract = {This book provides an introduction to statistical learning methods. It is aimed for upper level undergraduate students, masters students and Ph.D. students in the non-mathematical sciences. The book also contains a number of R labs with detailed explanations on how to implement the various methods in real life settings, and should be a valuable resource for a practicing data scientist.},
address = {New York, NY},
author = {James, Gareth and Witten, Daniela and Hastie, Trevor and Tibshirani, Robert},
booktitle = {Synthesis Lectures on Mathematics and Statistics},
doi = {10.1007/978-1-4614-7138-7},
editor = {Casella, G. and Fienberg, S. and Olkin, I.},
file = {:C$\backslash$:/Users/HP USER/Documents/repos/thesis/methodology/references/James (2013) An Introduction to Statistical Learning.pdf:pdf},
isbn = {978-1-4614-7137-0},
issn = {1938-1743},
pages = {1--426},
publisher = {Springer New York},
series = {Springer Texts in Statistics},
title = {{An Introduction to Statistical Learning}},
url = {http://link.springer.com/10.1007/978-1-4614-7138-7},
year = {2013}
}
@article{Kelly1994,
abstract = {Transportation decisions clearly affect land-use patterns, and land-use decisions clearly affect transportation systems. Urban theorists have addressed the cyclical land-usetransportation relationship for many decades and economists have modeled it extensively. Field studies demonstrate what the economists have predicted and what many theorists have feared: that, in many ways, highways shape urban areas. Yet little of that knowledge has found its way into planning practice, and land-use planning and transportation planning remain separate decision-making processes. Now that Congress has mandated that transportation planners consider both land-use plans and the land-use impacts of their decisions, the literature of planning practice should draw on the theoretical and research literature and provide guidance to planners on how to manage the transportation-land-use cycle.},
author = {Kelly, Eric Damian},
file = {:C$\backslash$:/Users/HP USER/Documents/repos/thesis/methodology/references/Kelly (1994) The Transportation Land Use Link.pdf:pdf},
journal = {Journal of Planning Literature},
number = {2},
pages = {p.128--145},
title = {{The Transportation Land Use Link}},
url = {https://journals-sagepub-com.myaccess.library.utoronto.ca/doi/10.1177/088541229400900202},
volume = {9},
year = {1994}
}
@inproceedings{Kohavi1995,
abstract = {We review accuracy estimation methods and compare the two most common methods: cross-validation and bootstrap. Recent experimental results on artificial data and theoretical results in restricted settings have shown that for selecting a good classifier from a set of classifiers (model selection) ten-fold cross-validation may be better than the more expensive leave-one-out cross-validation. We report on a large-scale experiment--over half a million runs of C4.5 and a Naive-Bayes algorithm--to estimate the effects of different parameters on these algorithms on real-world datasets. For cross-validation, we vary the number of folds and whether the folds are stratified or not; for bootstrap, we vary the number of bootstrap samples. Our results indicate that for real-word datasets similar to ours, the best method to use for model selection is ten-fold stratified cross validation, even if computation power allows using more folds.},
author = {Kohavi, Ron},
booktitle = {International Joint Conference on Artificial Intelligence (IJCAI)},
doi = {10.1067/mod.2000.109032},
file = {:C$\backslash$:/Users/HP USER/Documents/repos/thesis/methodology/references/Kohavi (1995) A Study of Cross-Validation and Bootstrap for Accuracy Estimation and Model Selection.pdf:pdf},
issn = {08895406},
title = {{A Study of Cross-Validation and Bootstrap for Accuracy Estimation and Model Selection}},
year = {1995}
}
@article{Lazer2017,
abstract = {Social life increasingly occurs in digital environments and continues to be mediated by digital systems. Big data represents the data being generated by the digitization of social life, which we break down into three domains: digital life, digital traces, and digitalized life. We argue that there is enormous potential in using big data to study a variety of phenomena that remain difficult to observe. However, there are some recurring vulnerabilities that should be addressed. We also outline the role institutions must play in clarifying the ethical rules of the road. Finally, we conclude by pointing to a number of nascent but important trends in the use of big data.},
author = {Lazer, David and Radford, J.},
doi = {10.1146/annurev-soc-060116-053457},
file = {:C$\backslash$:/Users/HP USER/Documents/Thesis/Data science papers/2017{\_}-{\_}David{\_}Lazer{\_}-{\_}DataexMachinaIntroductiontoBigData[retrieved{\_}2018-10-26].pdf:pdf},
isbn = {978-0-8243-2243-4},
issn = {0360-0572},
journal = {Ssrn},
pages = {19--39},
title = {{Data Ex Machina: Introduction to Big Data}},
year = {2017}
}
@techreport{Lowry1964,
abstract = {This report describes a computer model of the spatial organization of human activities within a metropolitan area. The model is intended for eventual use as 1) a device for evaluating the impact of public decisions (e.g., concerning urban renewal, tax policies, land-use controls, transportation investments) on metropolitan form; and 2) a device for predicting changes in metropolitan form which will follow over time as a consequence of currently visible or anticipated changes in key variables such as the pattern of 'basic' employment, the efficiency of the transportation system, or the growth of population. The development of the model to the stage at which it becomes a practical tool for planning and decision-making will require more time and effort. The first-generation model described in this report has been fitted to data for Pittsburgh, Pennsylvania, and enough experimental computer-runs have been completed to allow an appraisal of the practicability of my approach, and to indicate the strong and weak points of the model and also of the available data. The findings reported here offer guidance both to the development of a second-generation model and to data-collection programs. (Author).},
address = {Santa Monica CA},
author = {Lowry, Ira S},
file = {:C$\backslash$:/Users/HP USER/Documents/repos/thesis/methodology/references/Lowry (1964) A Model of Metropolis.pdf:pdf},
institution = {Rand Corporation},
title = {{A Model of Metropolis}},
url = {https://www.rand.org/content/dam/rand/pubs/research{\_}memoranda/2006/RM4035.pdf},
year = {1964}
}
@misc{MapandDataLibrary2019,
abstract = {“Census geography” refers to the geographic units used by Statistics Canada for disseminating information about the Census of Canada. Here are the definitions for each of the following types of units.},
author = {{Map and Data Library}},
booktitle = {University of Toronto},
title = {{Canadian census geography (unit) definitions}},
url = {https://mdl.library.utoronto.ca/canadian-census-geography-unit-definitions},
year = {2019}
}
@article{Martinez1992,
abstract = {Alonso's bid-rent theory and the discrete-choice random-utility theory appear in the literature as well-established alternative frameworks to model urban land use. As both approaches share the support of microeconomic theory, the main issue addressed in this paper is the theoretical comparison of the two approaches. It is demonstrated that in perfectly competitive land markets these approaches are equivalent, therefore they should be understood as complementary rather than alternative. The case of markets subject to speculative land prices is then explored for the cases of speculative supply and/or speculative demand, with the discovery that both approaches are theoretically equivalent in every case studied, thus extending the previous conclusion for the general case. These conclusions provide the base for an integrated and more comprehensive urban economic theory and for the bid-choice land-use model. 1},
author = {Martinez, F J},
doi = {10.1.1.459.5439},
file = {:C$\backslash$:/Users/HP USER/Documents/repos/thesis/methodology/references/Martinez (1992) The bid-choice land-use model .pdf:pdf},
journal = {Environment and Planning},
number = {January 1991},
pages = {871--885},
title = {{The bid-choice land-use model: an integrated economic framework}},
url = {http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.459.5439{\&}rep=rep1{\&}type=pdf},
volume = {24},
year = {1992}
}
@article{McCulloch1990a,
abstract = {Because of the "all-or-none" character of nervous activity, neural events and the relations among them can be treated by means of propositional logic. It is found that the behavior of every net can be described in these terms, with the addition of more complicated logical means for nets containing circles; and that for any logical expression satisfying certain conditions, one can find a net behaving in the fashion it describes. It is shown that many particular choices among possible neurophysiological assumptions are equivalent, in the sense that for every net behaving under one assumption, there exists another net which behaves under the other and gives the same results, although perhaps not in the same time. Various applications of the calculus are discussed,},
author = {McCulloch, Warren S and Pitts, Walter},
doi = {Doi 10.1016/S0092-8240(05)80006-0},
file = {:C$\backslash$:/Users/HP USER/Documents/repos/nhl{\_}draft/references/pdf/1990{\_}-{\_}Warren{\_}McCulloch{\_}-{\_}Alogicalcalculusoftheideasimmanentinnervousactivit[retrieved{\_}2019-04-06].pdf:pdf},
isbn = {0092-8240},
issn = {0092-8240},
journal = {Bulletin of Mathematical Biology},
number = {1--2},
pages = {99--115},
title = {{A logical calculus of the ideas immanent in nervous activity (reprinted from bulletin of mathematical biophysics, vol 5, pg 115-133, 1943)}},
url = {http://journals2.scholarsportal.info/pdf/00928240/v52i1-2/99{\_}alcotiiina.xml},
volume = {52},
year = {1990}
}
@techreport{McKean2015,
abstract = {A practical cross-border insight into real estate law.},
address = {London, UK},
author = {McKean, Heather and {Di Cresce}, Stella},
institution = {Global Legal Group},
isbn = {9781910083314},
keywords = {canada,real estate law},
title = {{The International Comparative Legal Guide to: Real Estate 2015, Chapter 6: Canada}},
url = {https://www.osler.com/osler/media/Osler/reports/real-estate/Real-Estate-Law-in-Canada.pdf},
year = {2015}
}
@article{Miller2018b,
abstract = {Integrated urban models (IUMs) (aka, integrated transport/land-use models) have been developed and (sometimes) applied for more than 50 years, dating back to the early 1960s. IUMs have been criticized over this same period on both practical and theoretical grounds. At the same time, continuing and very significant technological developments have made possible the development, implementation and use of such models in operational planning settings in various countries worldwide. A major review of the IUM state of the art and recommendations for evolution of this state were prepared by the author and colleagues 20 years ago. This paper presents an update of the 1998 report in terms of a summary of progress over the past 20 years, a critical assessment of the current IUM state of the art and practice, and needs and prospects for future development. This paper argues that the current modeling state is in “the doldrums,” similar to concerns raised by Pas in the seminal 1990 critique of activity-based travel models. It then outlines research and development needs to exploit current and emerging data, computing, and methodological developments that hold promise for the development of a much more powerful and useful “next generation” of IUMs.},
author = {Miller, Eric J},
doi = {10.5198/jtlu.2018.1273},
file = {:C$\backslash$:/Users/HP USER/Documents/repos/thesis/methodology/references/Miller (2018) Integrated urban modeling - past, present, future.pdf:pdf},
issn = {19387849},
journal = {Journal of Transport and Land Use},
number = {1},
pages = {387--399},
title = {{Integrated urban modeling: Past, present, and future}},
volume = {11},
year = {2018}
}
@misc{Miller2018c,
abstract = {Presentation given at 3rd Annual iCity-ORF Research Day University of Toronto June 22, 2018 What is an integrated urban model? Why integrated models? Agent-based microsimulation (ABM) ILUTE The ILUTE Reboot},
address = {Toronto},
author = {Miller, Eric J},
file = {:C$\backslash$:/Users/HP USER/Documents/repos/thesis/methodology/references/Miller (2018) ILUTE Model Reboot.pdf:pdf},
title = {{Towards the Next Generation of Integrated Urban Models}},
url = {https://uttri.utoronto.ca/files/2018/07/ILUTE-Integrated-Land-Use-Transportation-and-Environment-Model-Reboot.pdf},
year = {2018}
}
@article{Miller2018a,
abstract = {The primary objective of this paper is to “make the case” for adoption of microsimulation frameworks for development of integrated urban models. Similar to the case of activity-based travel models, microsimulation in integrated urban models enables such models to deal better with: heterogeneity and non-linearity in behavior; identification of the detailed spatial and socioeconomic distribution of impacts, benefits and costs; tracing complex interactions across agents and over time; providing support for modelling memory, learning and adaptation among agents; computational efficiency; and emergent behavior. The paper discusses strengths, weaknesses and challenges in microsimulating urban regions, including the extent to which microsimulation models are still subject to Lee's famous “seven sins of large-scale modelling,” as well as the extent to which they may help alleviate or reduce these sins in operational models. The paper concludes with a very brief discussion of future prospects for such models.},
author = {Miller, Eric J},
doi = {10.5198/jtlu.2018.1257},
file = {:C$\backslash$:/Users/HP USER/Documents/repos/thesis/methodology/references/Miller (2018) The case for microsimulation frameworks for integrated urban models.pdf:pdf},
journal = {Journal of Transport and Land Use},
number = {1},
pages = {1025--1037},
title = {{The case for microsimulation frameworks for integrated urban models}},
volume = {11},
year = {2018}
}
@incollection{Miller2019,
author = {Miller, Eric J},
booktitle = {Mapping the Travel Behavior Genome, The Role of Disruptive Technologies, Automation and Experimentation},
chapter = {2},
editor = {Goulias, K.G. and Davis, A.W.},
title = {{Travel Demand Models, The Next Generation: Boldly Going Where No-One Has Gone Before}},
year = {2019}
}
@article{Miller2011,
abstract = {The Integrated Land Use, Transportation, Environment (ILUTE) model system is an agent-based microsimulation model for the greater Toronto-Hamilton, Ontario, Canada, area. The model system uses disaggregate models of spatial socioeconomic processes to evolve the state of the greater Toronto-Hamilton area from a known base case to a predicted end state in 1-year time steps. ILUTE has reached a state of operational implementation in which historical validation runs are being undertaken. The model runs start with 100{\%} of the population of people, families, households, and dwelling units in the greater Toronto area that was synthesized for the year 1986. Twenty-year historical simulations (1986 to 2006) have been run, with model outputs being compared with Canadian census data and Transportation Tomorrow Survey data for 1991, 1996, 2001, and 2006. This paper presents recent findings from these historical validation tests and emphasizes the system's modeling of the demographic evolution of the population and the region's housing market.},
author = {Miller, Eric J and Farooq, Bilal and Chingcuanco, Franco and Wang, David},
doi = {10.3141/2255-10},
file = {:C$\backslash$:/Users/HP USER/Documents/repos/thesis/methodology/references/Miller (2011) Historical Validation of an Integrated Transport – Land Use Model System.pdf:pdf},
issn = {03611981},
journal = {Transportation Research Record},
number = {2255},
pages = {91--99},
title = {{Historical validation of integrated transport-land use model system}},
year = {2011}
}
@book{Miller1998,
abstract = {The nation's growth and the need to meet mobility, environmental, and energy objectives place demands on public transit systems. Current systems, some of which are old and in need of upgrading, must expand service area, increase service frequency, and improve efficiency to serve these demands. Research is necessary to solve operating problems, to adapt appropriate new technologies from other industries, and to introduce innovations into the transit industry. The Transit Cooperative Research Program (TCRP) serves as one of the principal means by which the transit industry can develop innovative near-term solutions to meet demands placed on it. The need for TCRP was originally identified in TRB Special Report 213—Research for Public Transit: New Directions, published in 1987 and based on a study sponsored by the Urban Project H-12 FY'96 ISSN 1073-4872 ISBN 0-309-06324-8 Library of Congress Catalog Card No. 99-71031 . 1999 Transportation Research Board Price {\$}22.00ID - 1535},
author = {Miller, Eric J and Kriger, David S and Hunt, John Douglas},
booktitle = {TCRP Report 48},
doi = {10.17226/9435},
file = {:C$\backslash$:/Users/HP USER/Documents/repos/thesis/methodology/references/Miller (1999) Integrated Urban Models for Simulation.pdf:pdf},
isbn = {0309063248},
pages = {1--32},
title = {{Integrated Urban Models for Simulation of Transit and Land Use Policies Guidelines for Implementation and Use}},
year = {1998}
}
@techreport{Mitchell1980,
abstract = {This paper defines precisely the notion of bias in generalization problems, then shows that biases are necessary for the inductive leap. Classes of justifiable biases are considered, and the relationship between bias and domain-independence is considered.},
address = {New Brunswick NJ},
author = {Mitchell, Tom M},
file = {:C$\backslash$:/Users/HP USER/Documents/repos/thesis/methodology/references/Mitchell (1980) Need For Biases in Learning Generalizations.pdf:pdf},
institution = {Computer Science Department, Rutgers University, New Brunswick NJ},
number = {Rutgers CS tech report CBM-TR-117},
title = {{The Need for Biases in Learning Generalizations}},
url = {https://www.cs.cmu.edu/{~}tom/pubs/NeedForBias{\_}1980.pdf},
year = {1980}
}
@article{Moeckel2017,
abstract = {Traditionally, integrated land-use/transportation models intend to represent all opportunities of travel and household location, maximize utilities and find an equilibrium in which no person or household could improve their satisfaction any further. Energy scarcity, higher transportation costs, and an increasing share of low-income households, on the other hand, demand special attention to represent constraints that households face, rather than opportunities for utility maximization. The integrated land-use model SILO explicitly represents various constraints, including the price of a dwelling, the travel time to work, and the monetary transportation budget. SILO ensures that no household makes choices that violate these constraints. Implementing such constraints helps SILO to generate more realistic results under scenarios that put current conditions under a stress test, such as a serious increase in transportation costs or severely increased congestion.},
author = {Moeckel, Rolf},
doi = {10.5198/jtlu.2016.810},
file = {:C$\backslash$:/Users/HP USER/Downloads/Moeckel (2016) Constraints in household relocation Modeling land-usetransport.pdf:pdf},
issn = {19387849},
journal = {Journal of Transport and Land Use},
keywords = {Housing and transportation budget,Land-use/transport interactions,Logit model,Microsimulation},
number = {1},
pages = {211--228},
title = {{Constraints in household relocation: Modeling land-use/transport interactions that respect time and monetary budgets}},
volume = {10},
year = {2017}
}
@article{Nisbet2018a,
author = {Nisbet, Robert and Miner, Gary and Yale, Ken},
doi = {10.1016/b978-0-12-416632-5.00020-7},
file = {:C$\backslash$:/Users/HP USER/Downloads/Nisbet (2018) Significance versus Luck in the Age of Mining - Issues of P-value.pdf:pdf},
isbn = {9780124166325},
journal = {Handbook of Statistical Analysis and Data Mining Applications},
pages = {753--765},
title = {{Significance versus Luck in the Age of Mining: The Issues of P -Value “Significance” and “Ways to Test Significance of Our Predictive Analytic Models”}},
year = {2018}
}
@incollection{Nisbet2018,
abstract = {In previous chapters, we focused on the performance of various major aspects of predictive analytic modeling and showed how this process can be used in several analytic applications. Novice modelers might consider taking shortcuts in performing the tasks listed in this chapter, but that is likely to lead to the development of an inadequate model or to no good model at all. It is a commonly held maxim in predictive analytics that 60{\%}–90{\%} of the project time will be consumed by data preparation tasks. In this chapter, we will put the “pieces” together for the bulk of the analytic project to show a step-by-step operation in the data preparation phase of CRISP-DM, which may be the most critical part of the project. This chapter will discuss some of the tasks in phases of the CRISP-DM process model, because these tasks are often part of data preparation.},
author = {Nisbet, Robert and Miner, Gary and Yale, Ken},
booktitle = {Handbook of Statistical Analysis and Data Mining Applications},
chapter = {Chapter 18},
doi = {10.1016/B978-0-12-416632-5.00018-9},
file = {:C$\backslash$:/Users/HP USER/Downloads/Nisbet (2018) A Data Preparation Cookbook - Handbook of Statistical Analysis and Data Mining Applications (Second Edition).pdf:pdf},
isbn = {9780124166325},
pages = {727--740},
publisher = {Elsevier},
title = {{A Data Preparation Cookbook}},
url = {https://linkinghub.elsevier.com/retrieve/pii/B9780124166325000189},
year = {2018}
}
@phdthesis{Noto2015,
abstract = {Public actors and, in general, public service providers - such as municipalities, provinces and agencies - play a major role in achieving a satisfying quality of life for citizens. The social context in which these institutions operate is intrinsically complex. This complexity arises from the different expectations of the community (pluralism). It also arises from the plurality of actors involved in policy-making (institutional complexity) and from the interplays of several factors, such as those related to technical, economic and environmental dimensions (scientific uncertainty) (Head {\&} Alford, 2013). The above characteristics determine what literature defines as ‘wicked' problems (Rittel {\&} Webber, 1973). The term ‘wicked' does not means ‘evil', but refers to issues that are hard to define and manage, due to the high complexity of the environment which they affect (Australian Public Service Commission, 2007; Head {\&} Alford, 2013). These issues often lead to counterintuitive behaviours in terms of time (trade-off between short- and long-term) and space (trade-off between different institutions or functional areas within an organization) when actions are taken to resolve them. Institutional complexity, pluralism and scientific uncertainty typically characterize the provision of public services, for this reason, many issues that Public Administrations (PA) have to deal with, can be considered wicked.},
author = {Noto, Guido and Cosenz, Federico and Bianchi, Carmine},
file = {:C$\backslash$:/Users/HP USER/Documents/repos/thesis/methodology/references/PhDthesis{\_}Guido{\_}Noto.pdf:pdf},
school = {University of Palermo},
title = {{Urban Transportation Governance And Wicked Problems: A Systemic And Performance Oriented Approach}},
type = {PhD thesis},
url = {https://iris.unipa.it/retrieve/handle/10447/161112/257955/PhDthesis{\_}Guido{\_}Noto.pdf},
year = {2015}
}
@article{scikit-learn,
abstract = {Scikit-learn is a Python module integrating a wide range of state-of-the-art machine learning algorithms for medium-scale supervised and unsupervised problems. This package focuses on bringing machine learning to non-specialists using a general-purpose high-level language. Emphasis is put on ease of use, performance, documentation, and API consistency. It has minimal dependencies and is distributed under the simplified BSD license, encouraging its use in both academic and commercial settings. Source code, binaries, and documentation can be downloaded from http://scikit-learn.sourceforge.net.},
author = {Pedregosa, F. and Varoquaux, G. and Gramfort, A. and Michel, V. and Thirion, B. and Grisel, O. and Blondel, M. and Prettenhofer, P. and Weiss, R. and Dubourg, V. and Vanderplas, J. and Passos, A. and Cournapeau, D. and Brucher, M. and Perrot, M. and Duchesnay, E.},
doi = {10.1145/2786984.2786995},
file = {:C$\backslash$:/Users/HP USER/Documents/repos/thesis/methodology/references/Pedregosa (2011) Scikit-learn Machine Learning in Python.pdf:pdf},
issn = {23750529},
journal = {Journal of Machine Learning Research},
pages = {2825----2830},
title = {{Scikit-learn: Machine Learning in {\{}P{\}}ython}},
url = {http://www.jmlr.org/papers/volume12/pedregosa11a/pedregosa11a.pdf},
volume = {12},
year = {2011}
}
@article{Preston2007,
abstract = {This paper briefly reviews the inexorable rise of the social exclusion policy paradigm and uses an adaptation of Amartya Sen's theory of entitlement to determine appropriate policy responses. In particular, the promotion by the UK Department for Transport of accessibility planning is examined. Although this initiative is not totally without merit, the resulting analysis may be too aggregate, both spatially and socially. The weakness of such an approach is that transport-related social exclusion is not always a socially and spatially concentrated process. Instead we suggest a matrix of area accessibility, area mobility and individual mobility as a possible schema for identifying concentrated and scattered manifestations of social exclusion and inclusion and for suggesting appropriate policy responses. This schema helps produce a more spatially and socially differentiated conceptualisation of social exclusion, helps identify policy responses and most critically highlights that the problems of the socially excluded immobile should not be analysed in isolation from the socially included mobile. {\textcopyright} 2006.},
author = {Preston, John and Raj{\'{e}}, Fiona},
doi = {10.1016/j.jtrangeo.2006.05.002},
file = {:C$\backslash$:/Users/HP USER/Documents/repos/thesis/methodology/references/Preston (2007) Accessibility, mobility and transport-related social exclusion.pdf:pdf},
issn = {09666923},
journal = {Journal of Transport Geography},
keywords = {Accessibility,Mobility,Social exclusion,Social inclusion},
number = {3},
pages = {151--160},
title = {{Accessibility, mobility and transport-related social exclusion}},
volume = {15},
year = {2007}
}
@misc{Raschka2014,
abstract = {Linear Discriminant Analysis (LDA) is most commonly used as dimensionality reduction technique in the pre-processing step for pattern-classification and machine learning applications. The goal is to project a dataset onto a lower-dimensional space with good class-separability in order avoid overfitting (“curse of dimensionality”) and also reduce computational costs. Ronald A. Fisher formulated the Linear Discriminant in 1936 (The Use of Multiple Measurements in Taxonomic Problems), and it also has some practical uses as classifier. The original Linear discriminant was described for a 2-class problem, and it was then later generalized as “multi-class Linear Discriminant Analysis” or “Multiple Discriminant Analysis” by C. R. Rao in 1948 (The utilization of multiple measurements in problems of biological classification)},
author = {Raschka, Sebastian},
booktitle = {https://sebastianraschka.com},
title = {{Linear Discriminant Analysis}},
url = {https://sebastianraschka.com/Articles/2014{\_}python{\_}lda.html},
year = {2014}
}
@article{Raschka2018,
abstract = {The correct use of model evaluation, model selection, and algorithm selection techniques is vital in academic machine learning research as well as in many industrial settings. This article reviews different techniques that can be used for each of these three subtasks and discusses the main advantages and disadvantages of each technique with references to theoretical and empirical studies. Further, recommendations are given to encourage best yet feasible practices in research and applications of machine learning. Common methods such as the holdout method for model evaluation and selection are covered, which are not recommended when working with small datasets. Different flavors of the bootstrap technique are introduced for estimating the uncertainty of performance estimates, as an alternative to confidence intervals via normal approximation if bootstrapping is computationally feasible. Common cross-validation techniques such as leave-one- out cross-validation and k-fold cross-validation are reviewed, the bias-variance trade-off for choosing k is discussed, and practical tips for the optimal choice of k are given based on empirical evidence. Different statistical tests for algorithm comparisons are presented, and strategies for dealing with multiple comparisons such as omnibus tests and multiple-comparison corrections are discussed. Finally, alternative methods for algorithm selection, such as the combined F-test 5x2 cross- validation and nested cross-validation, are recommended for comparing machine learning algorithms when datasets are small.},
archivePrefix = {arXiv},
arxivId = {1811.12808v2},
author = {Raschka, Sebastian},
eprint = {1811.12808v2},
file = {:C$\backslash$:/Users/HP USER/Documents/repos/thesis/methodology/references/Raschka (2018) Model Evaluation, Model Selection, and Algorithm.pdf:pdf},
institution = {University of Wisconsin–Madison},
journal = {arXiv e-prints},
keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
number = {arXiv:1811.12808},
pages = {arXiv:1811.12808},
title = {{Model Evaluation , Model Selection , and Algorithm Selection in Machine Learning}},
year = {2018}
}
@book{RaschkaMirjalili2017,
address = {Birmingham, UK},
author = {Raschka, Sebastian and Mirjalili, Vahid},
edition = {2},
isbn = {978-1787125933},
keywords = {Clustering,Data Science,Deep Learning,Machine Learning,Neural Networks,Programming,Supervised Learning},
publisher = {Packt Publishing},
title = {{Python Machine Learning, 2nd Ed.}},
year = {2017}
}
@article{Rittel1973,
abstract = {The search for sceintific bases for confronting problems of social policy is bound to fail, because of the nature of these problems. They are "wicked" problems, whereas science has developed to deal with "tame" problems. Policy problems cannot be definitively described. Moreover, in a pluralistic society there is nothing like the undisputable public good; there is no objective definition of equity; policies that respond to social problems cannot be meaningfully correct or false; and it makes no sense to talk about "optimal solutions" to social problems inless severe qualifications are imposed first. Even worse, there are no "solutions" in the sense of definitive and},
author = {Rittel, Horst W.J. and Webber, Melvin M.},
file = {:C$\backslash$:/Users/HP USER/Documents/repos/thesis/methodology/references/1973 Rittel and Webber Wicked Problems.pdf:pdf},
journal = {Policy Sciences},
number = {2},
pages = {155--169},
title = {{Dilemmas in a General Theory of Planning}},
volume = {4},
year = {1973}
}
@techreport{Rosenblatt1957a,
address = {Buffalo, NY},
author = {Rosenblatt, Frank},
file = {:C$\backslash$:/Users/HP USER/Documents/repos/nhl{\_}draft/references/pdf/rosenblatt-1957.pdf:pdf},
institution = {Cornell Aeronautical Laboratory, Inc.},
title = {{The Perceptron: a Percieving and Recognizing Automation}},
url = {https://blogs.umass.edu/brain-wars/files/2016/03/rosenblatt-1957.pdf},
year = {1957}
}
@article{Rosenfield2013,
abstract = {The Housing Market Evolutionary System (HoMES) is the updated housing market module for the Integrated Land Use, Transportation, Environment (ILUTE) model system. HoMES is a disaggregate, agent-based microsimulation of the owner- , location choices and valuations, the endogenous supply of housing by type and location, and the endogenous determination of sale prices and rents. The new model offers significant improvements over previous attempts by including a reformulated market clearing mechanism, market dependency on macro-economic conditions, and improved computational performance. A 100{\%} synthesized population is validated against historical data for the Greater Toronto-Hamilton Area. {\textcopyright} 2013 The Authors. Published by Elsevier B.V.},
author = {Rosenfield, Adam and Chingcuanco, Franco and Miller, Eric J},
doi = {10.1016/j.procs.2013.06.112},
file = {:C$\backslash$:/Users/HP USER/Documents/repos/thesis/methodology/references/Rosenfield (2013) Agent-based Housing Market Microsimulation for IUM.pdf:pdf},
issn = {18770509},
journal = {Procedia Computer Science},
keywords = {Agent-based,Disequilibrium,Housing,ILUTE,Integrated model,Market,Microsimulation},
pages = {841--846},
title = {{Agent-based housing market microsimulation for integrated land use, transportation, environment model system}},
volume = {19},
year = {2013}
}
@misc{Scikit-learndevelopers2019,
abstract = {Comparison of the sparsity (percentage of zero coefficients) of solutions when L1, L2 and Elastic-Net penalty are used for different values of C. We can see that large values of C give more freedom to the model. Conversely, smaller values of C constrain the model more. In the L1 penalty case, this leads to sparser solutions. As expected, the Elastic-Net penalty sparsity is between that of L1 and L2.},
author = {Scikit-learn developers},
booktitle = {Online Documentation for scikit-learn},
title = {{L1 Penalty and Sparsity in Logistic Regression}},
url = {https://scikit-learn.org/stable/auto{\_}examples/linear{\_}model/plot{\_}logistic{\_}l1{\_}l2{\_}sparsity.html},
year = {2019}
}
@misc{Scikit-learndevelopers2019a,
abstract = {Learning the parameters of a prediction function and testing it on the same data is a methodological mistake: a model that would just repeat the labels of the samples that it has just seen would have a perfect score but would fail to predict anything useful on yet-unseen data. This situation is called overfitting. To avoid it, it is common practice when performing a (supervised) machine learning experiment to hold out part of the available data as a test set X{\_}test, y{\_}test. Note that the word “experiment” is not intended to denote academic use only, because even in commercial settings machine learning usually starts out experimentally. Here is a flowchart of typical cross validation workflow in model training. The best parameters can be determined by grid search techniques.},
author = {Scikit-learn developers},
booktitle = {Online Documentation for scikit-learn},
title = {{Cross-validation: evaluating estimator performance}},
url = {https://scikit-learn.org/stable/modules/cross{\_}validation.html{\#}cross-validation},
year = {2019}
}
@misc{Scikit-learndevelopers2019b,
abstract = {Scalers are linear (or more precisely affine) transformers and differ from each other in the way to estimate the parameters used to shift and scale each feature. Unscaled data can also slow down or even prevent the convergence of many gradient-based estimators. Many estimators are designed with the assumption that each feature takes values close to zero or more importantly that all features vary on comparable scales. In particular, metric-based and gradient-based estimators often assume approximately standardized data (centered features with unit variances). A notable exception are decision tree-based estimators that are robust to arbitrary scaling of the data. This example uses different scalers, transformers, and normalizers to bring the data within a pre-defined range.},
author = {Scikit-learn developers},
booktitle = {Online Documentation for scikit-learn},
title = {{Compare the effect of different scalers on data with outliers}},
url = {https://scikit-learn.org/stable/auto{\_}examples/preprocessing/plot{\_}all{\_}scaling.html{\#}sphx-glr-auto-examples-preprocessing-plot-all-scaling-py},
year = {2019}
}
@misc{Scikit-learndevelopers2019c,
abstract = {The principle behind nearest neighbor methods is to find a predefined number of training samples closest in distance to the new point, and predict the label from these. The number of samples can be a user-defined constant (k-nearest neighbor learning), or vary based on the local density of points (radius-based neighbor learning). The distance can, in general, be any metric measure: standard Euclidean distance is the most common choice. Neighbors-based methods are known as non-generalizing machine learning methods, since they simply “remember” all of its training data (possibly transformed into a fast indexing structure such as a Ball Tree or KD Tree).},
author = {Scikit-learn developers},
booktitle = {Online Documentation for scikit-learn},
title = {{Nearest Neighbors}},
url = {https://scikit-learn.org/stable/modules/neighbors.html},
year = {2019}
}
@article{Shearer2000,
abstract = {This article describes CRISP-DM (CRoss-Industry Standard Process for Data Mining), a non-proprietary, documented, and freely available data mining model. Developed by industry leaders with input from more than 200 data mining users and data mining tool and service providers, CRISP-DM is an industry-, tool-, and application-neutral model. This model encourages best practices and offers organizations the structure needed to realize better, faster results from data mining. CRISP-DM organizes the data mining process into six phases: business understanding, data understanding, data preparation, modeling, evaluation, and deployment. These phases help organizations understand the data mining process and provide a road map to follow while planning and carrying out a data mining project. This article explores all six phases, including the tasks involved with each phase. Sidebar material, which takes a look at specific data mining problem types and techniques for addressing them, is provided.},
author = {Shearer, Colin},
file = {:C$\backslash$:/Users/HP USER/Documents/repos/thesis/methodology/references/the-crisp-dm-model-the-new-blueprint-for-data-mining-shearer-colin.pdf:pdf},
journal = {Journal of Data Warehousing},
number = {4},
pages = {13--22},
title = {{The CRISP-DM Model: The New Blueprint for Data Mining}},
url = {http://scholar.google.de/scholar?hl=de{\&}q=.+The+CRISP-DM+model{\%}3A+the+new+blueprint+for+data+mining{\&}btnG=Suche{\&}lr={\&}as{\_}ylo={\&}as{\_}vis=0{\#}0},
volume = {5},
year = {2000}
}
@article{Sherry1999,
abstract = {This article reviews empirical studies of the relationship between transportation facilities—highways, heavy rail, and light rail transit systems—and property values. The main objective is to develop an explanation for inconsistent results presented in this literature over the past several decades. Results from these studies vary based on whether travel time or travel distance is used as a measure of accessibility. When researchers measure access to highways and rail transit in terms of travel time, study results usually indicate the expected inverse relationship between access to transportation facilities and property values. When studies use travel distance as a measure of access to transportation facilities, results tend to show mixed property value effects. The delineation of study areas also appears to influence the direction of results. This article offers a new interpretation of the transportation facility-property value literature that improves our ability to measure this relationship and to anticipate land-market responses to transportation facilities.},
author = {Sherry, Ryan},
doi = {10.1177/08854129922092487},
file = {:C$\backslash$:/Users/HP USER/Documents/repos/thesis/methodology/references/Sherry (1999) Property Values and Transportation Facilities.pdf:pdf},
issn = {08854122},
journal = {Journal of Planning Literature},
number = {4},
pages = {412--427},
title = {{Property values and transportation facilities: Finding the transportation-land use connection}},
volume = {13},
year = {1999}
}
@misc{StatisticsCanada2015,
abstract = {A dissemination area (DA) is a small, relatively stable geographic unit composed of one or more adjacent dissemination blocks. It is the smallest standard geographic area for which all census data are disseminated. DAs cover all the territory of Canada.},
author = {{Statistics Canada}},
booktitle = {Census Program Reference materials, 2011 Census Dictionary},
title = {{Dissemination area (DA)}},
url = {https://www12.statcan.gc.ca/census-recensement/2011/ref/dict/geo021-eng.cfm},
year = {2015}
}
@misc{StatisticsCanada2018,
abstract = {This diagram shows the hierarchy of the standard geographic units for the dissemination of the 2011 Census of Canada. It indicates whether the geographic unit is an administrative or statistical area. Please refer to the definitions in the geography universe index of the 2011 Census Dictionary.},
author = {{Statistics Canada}},
booktitle = {Illustrated Glossary 92-195-X},
title = {{Hierarchy of standard geographic units}},
url = {https://www150.statcan.gc.ca/n1/pub/92-195-x/2011001/other-autre/hierarch/h-eng.htm},
year = {2018}
}
@book{Stover1988,
author = {Stover, Vergil G and Koepke, Frank J},
isbn = {0139304134},
publisher = {Pearson College Div},
title = {{Transportation and Land Development}},
year = {1988}
}
@techreport{TeranetEnterprisesInc.2011,
abstract = {The Province of Ontario Land Registration Information System (POLARIS{\textregistered}) is the computerized system that stores and manages ownership data for each property in Ontario which have been automated into the Electronic Land Registration System (ELRS{\textregistered}). All of the information in the POLARIS databases is compiled from the actual Land Registry Office (“LRO”) records. The Ownership Property Report contains property-based attributes for properties in the defined geographic coverage. The POLARIS Property Identification Number (PIN) is the key for each record.},
address = {Toronto},
author = {{Teranet Enterprises Inc.}},
institution = {Teranet Enterprises Inc.},
title = {{Product Description: Ownership Property Report}},
year = {2011}
}
@misc{TeranetEnterprisesInc.2019,
abstract = {Teranet is the exclusive provider of Ontario's online property search and registration. We developed, own and operate the Ontario Electronic Land Registration System – one of the most advanced, secure and sophisticated land registration systems in the world. Teranet also provides online access to Ontario's Writs System.},
author = {{Teranet Enterprises Inc.}},
booktitle = {www.teranet.ca},
title = {https://www.teranet.ca},
url = {https://www.teranet.ca},
year = {2019}
}
@misc{TeranetEnterprisesInc.,
abstract = {Historically, searching titles in Ontario was a complicated and time consuming activity. Each search required a personal visit to a Land Registry Office to view paper indexes, original documents and plans. In 1985, the Government of Ontario initiated the Province of Ontario Land Registration Information System (POLARIS) pilot project for the purposes of records automation and the conversion from the Registry System to the Land Titles System. The Land Registration Reform Act (Ontario) was introduced to facilitate electronic search and registration of properties and the automation of paper-based records. POLARIS was built by the Province to house and process electronic land records. Teranet converted all qualified Registry properties in Ontario to the Land Titles system and automated existing paper Land Titles parcels. As a result, 99.9{\%} of property in Ontario is parcelized and administered under the Land Titles system, which affords a property ownership guarantee by the province.},
author = {{Teranet Enterprises Inc.}},
booktitle = {www.teranet.ca},
title = {{About POLARIS, Teranet}},
url = {https://www.teranet.ca/registry-solutions/about-polaris/},
year = {2019}
}
@misc{TheGovernmentofOntario1990,
abstract = {This Act provides for changes in land registration, it consists of 32 sections and is divided into three parts. Part I concerns documents, and changes in the requirements for land registration. Part II is dedicated to automated recording and property mapping and Part III covers electronic registration and related matters.},
address = {Toronto},
author = {{The Government of Ontario}},
publisher = {The Government of Ontario},
title = {{Land Registration Reform Act, R.S.O. 1990, c. L.4}},
url = {https://www.ontario.ca/laws/statute/90l04},
year = {1990}
}
@article{VanLierop2017,
abstract = {{\textcopyright} 2018 Taylor  {\&}  Francis. Land use and transportation have a long and complex history of influencing one another, and this relationship has been studied through many dimensions over the past hundred years. The role of land use and transportation researchers has been, and still is, to develop precisely measured, relevant, and reflective research that can help transport and planning professionals design effective policies and plans. While researchers in this domain often conduct studies that lead to changes in planning practice, their areas of research focus are constantly evolving and frequently influenced by the technologies, policies, and implementation of particular approaches to planning within a given context and time. The relationship between land use and transportation research and practice is therefore cyclical: practice is informed by research, and research interests stem from practical issues and trends.},
author = {{Van Lierop}, Dea and Boisjoly, Genevi{\`{e}}ve and Gris{\'{e}}, Emily and El-Geneidy, Ahmed},
doi = {10.4324/9781315308715},
file = {:C$\backslash$:/Users/HP USER/Documents/repos/thesis/methodology/references/van Lierop (2017) Evolution in Land Use and Transportation Research.pdf:pdf},
isbn = {9781315308708},
journal = {Planning Knowledge and Research},
pages = {110--131},
title = {{Evolution in land use and transportation research}},
volume = {1825},
year = {2017}
}
@article{Wickham2014,
abstract = {In this paper we present the R package gRain for propagation in graphical indepen- dence networks (for which Bayesian networks is a special instance). The paper includes a description of the theory behind the computations. The main part of the paper is an illustration of how to use the package. The paper also illustrates how to turn a graphical model and data into an independence network.},
archivePrefix = {arXiv},
arxivId = {arXiv:1501.0228},
author = {Wickham, Hadley},
doi = {10.18637/jss.v059.i10},
eprint = {arXiv:1501.0228},
file = {:C$\backslash$:/Users/HP USER/Documents/Thesis/Data science papers/Tidy{\_}Data{\_}v59i10.pdf:pdf},
isbn = {9780387781662},
issn = {1548-7660},
journal = {Journal of Statistical Software},
keywords = {data cleaning,data tidying,r,relational databases},
number = {10},
pmid = {18291371},
title = {{Tidy Data}},
url = {http://www.jstatsoft.org/v59/i10/},
volume = {59},
year = {2014}
}
@article{Wolpert1996,
abstract = {This is the first of two papers that use off-training set {\{}(OTS){\}} error to investigate the assumption-free relationship between learning algorithms. This first paper discusses the senses in which there are no {\{}$\backslash$textbackslash{\}}textita priori distinctions between learning algorithms. {\{}(The{\}} second paper discusses the senses in which there are such distinctions.) In this first paper it is shown, loosely speaking, that for any two algorithms A and B, there are “as many” targets (or priors over targets) for which A has lower expected {\{}OTS{\}} error than B as vice-versa, for loss functions like zero-one loss. In particular, this is true if A is cross-validation and B is “anti-cross-validation” (choose the learning algorithm with largest cross-validation error). This paper ends with a discussion of the implications of these results for computational learning theory. It is shown that one can not say: if empirical misclassification rate is low; the {\{}Vapnik-Chervonenkis{\}} dimension of your generalizer is small; and the training set is large, then with high probability your {\{}OTS{\}} error is small. Other implications for “membership queries” algorithms and “punting” algorithms are also discussed.},
author = {Wolpert, David H},
file = {:C$\backslash$:/Users/HP USER/Documents/Thesis/Data science papers/Wolpert(1996) - The Lack of A Priori Distinctions Between Learning Algorithms.pdf:pdf},
journal = {Neural Computation},
number = {7},
pages = {1391--1420},
title = {{The Lack of A Priori Distinctions Between Learning Algorithms}},
volume = {8},
year = {1996}
}
@article{Wolpert1992,
abstract = {This paper introduces stacked generalization, a scheme for minimizing the generalization error rate of one or more generalizers. Stacked generalization works by deducing the biases of the generalizer(s) with respect to a provided learning set. This deduction proceeds by generalizing in a second space whose inputs are (for example) the guesses of the original generalizers when taught with part of the learning set and trying to guess the rest of it, and whose output is (for example) the correct guess. When used with multiple generalizers, stacked generalization can be seen as a more sophisticated version of cross-validation, exploiting a strategy more sophisticated than cross-validation's crude winner-takes-all for combining the individual generalizers. When used with a single generalizer, stacked generalization is a scheme for estimating (and then correcting for) the error of a generalizer which has been trained on a particular learning set and then asked a particular question. After introducing stacked generalization and justifying its use, this paper presents two numerical experiments. The first demonstrates how stacked generalization improves upon a set of separate generalizers for the NETtalk task of translating text to phonemes. The second demonstrates how stacked generalization improves the performance of a single surface-fitter. With the other experimental evidence in the literature, the usual arguments supporting cross-validation, and the abstract justifications presented in this paper, the conclusion is that for almost any real-world generalization problem one should use some version of stacked generalization to minimize the generalization error rate. This paper ends by discussing some of the variations of stacked generalization, and how it touches on other fields like chaos theory. {\textcopyright} 1992 Pergamon Press Ltd.},
author = {Wolpert, David H.},
doi = {10.1016/S0893-6080(05)80023-1},
file = {:C$\backslash$:/Users/HP USER/Documents/repos/thesis/methodology/references/Wolpert (1992) Stacked Generalization.pdf:pdf},
issn = {08936080},
journal = {Neural Networks},
keywords = {Combining generalizers,Error estimation and correction,Generalization and induction,Learning set preprocessing,cross-validation},
number = {2},
pages = {241--259},
title = {{Stacked generalization}},
volume = {5},
year = {1992}
}
@techreport{Wolpert1997,
abstract = {A framework is developed to explore the connection between effective optimization algorithms and the problems they are solving. A number of "no free lunch" (NFL) theorems are presented which establish that for any algorithm, any elevated performance over one class of problems is offset by performance over another class. These theorems result in a geometric interpretation of what it means for an algorithm to be well suited to an optimization problem. Applications of the NFL theorems to information-theoretic aspects of optimization and benchmark measures of performance are also presented. Other issues addressed include time-varying optimization problems and a priori "head-to-head" minimax distinctions between optimization algorithms, distinctions that result despite the NFL theorems' enforcing of a type of uniformity over all algorithms.},
author = {Wolpert, David H and Macready, William G},
booktitle = {IEEE TRANSACTIONS ON EVOLUTIONARY COMPUTATION},
file = {:C$\backslash$:/Users/HP USER/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Wolpert, Macready - 1997 - No Free Lunch Theorems for Optimization.pdf:pdf},
keywords = {Index Terms-Evolutionary algorithms,information theory,optimization},
number = {1},
pages = {67},
title = {{No Free Lunch Theorems for Optimization}},
url = {https://ti.arc.nasa.gov/m/profile/dhw/papers/78.pdf},
volume = {1},
year = {1997}
}
