{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GTHA housing market database\n",
    "# OSEMN methodology Step 4: Model\n",
    "# Predict land use from new features in Teranet\n",
    "# Linear Discriminant Analysis (LDA)\n",
    "---\n",
    "\n",
    "This notebook describes _Step 4: Model_ of OSEMN methodology performed on the Teranet dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import os\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis as LDA\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler, LabelEncoder\n",
    "from sklearn.linear_model import Perceptron, LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier, export_graphviz\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "# from pydotplus import graph_from_dot_data\n",
    "from time import time\n",
    "\n",
    "sns.set()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "sys.path.append('../../../../src')\n",
    "\n",
    "from io_utils import df_from_csv\n",
    "from plot_utils import plot_hist\n",
    "from model_utils import fit_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Teranet data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = '../../../../data/'\n",
    "teranet_path = data_path + 'teranet/'\n",
    "os.listdir(teranet_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "teranet_df = df_from_csv(teranet_path + '4_Teranet_lu_predict.csv', parse_dates=['registration_date'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Census data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "census_path = data_path + 'census/'\n",
    "os.listdir(census_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "census_df = df_from_csv(census_path + 'da_census_select_tidy.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load TTS data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tts_path = data_path + 'tts/'\n",
    "os.listdir(tts_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tts_df = df_from_csv(tts_path + 'taz_tts_tidy.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Join Census data to Teranet data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = time()\n",
    "df = pd.merge(teranet_df, census_df, how='left',\n",
    "              left_on=['dauid', 'census2001_year'], right_on=['DAUID', 'year'])\n",
    "df = df.drop(['DAUID', 'year_y'], axis=1).rename(columns={'year_x': 'year'})\n",
    "elapsed = time() - t\n",
    "print(\"----- Census variables were joined to the DataFrame\"\n",
    "      \"\\nin {0:,.2f} seconds ({1:.2f} minutes)\".format(elapsed, elapsed / 60) + \n",
    "      \"\\nwith {0:,} rows\\nand {1:,} columns\"\n",
    "      .format(df.shape[0], df.shape[1]) + \n",
    "      \"\\n-- Column names:\\n\", df.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Join TTS data to Teranet data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = time()\n",
    "df = pd.merge(df, tts_df, how='left',\n",
    "              left_on=['taz_o', 'tts_year'], right_on=['TAZ_O', 'year'])\n",
    "df = df.drop(['taz_o', 'year_y'], axis=1).rename(columns={'year_x': 'year'})\n",
    "elapsed = time() - t\n",
    "print(\"----- TTS variables were joined to the DataFrame\"\n",
    "      \"\\nin {0:,.2f} seconds ({1:.2f} minutes)\".format(elapsed, elapsed / 60) + \n",
    "      \"\\nwith {0:,} rows\\nand {1:,} columns\"\n",
    "      .format(df.shape[0], df.shape[1]) + \n",
    "      \"\\n-- Column names:\\n\", df.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Take a train-test subset of Teranet records"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "year_train_test = 2012\n",
    "year_val1 = 2011\n",
    "year_val2 = 2013\n",
    "\n",
    "s = df.query('year == @year_train_test')\n",
    "s_val1 = df.query('year == @year_val1')\n",
    "s_val2 = df.query('year == @year_val2')\n",
    "\n",
    "print(\"{0:,} Teranet records in the train-test subset (records from {1}).\".format(len(s), year_train_test))\n",
    "print(\"{0:,} Teranet records in the validation subset #1 (records from {1}).\".format(len(s_val1), year_val1))\n",
    "print(\"{0:,} Teranet records in the validation subset #2 (records from {1}).\".format(len(s_val2), year_val2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Select features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_feat_cols = ['pin_total_sales', 'xy_total_sales', 'pin_prev_sales', 'xy_prev_sales',\n",
    "                 'xy_first_sale', 'pin_years_since_last_sale',\n",
    "                 'xy_years_since_last_sale', 'xy_years_to_next_sale',\n",
    "                 'da_days_since_last_sale', 'da_years_since_last_sale',\n",
    "                 'xy_sale_next_6m', 'price_2016', 'pin_price_cum_sum',\n",
    "                 'xy_price_cum_sum', 'pin_price_pct_change', 'xy_price_pct_change',\n",
    "                 'price_da_pct_change', 'med_price_xy', 'med_price_year',\n",
    "                 'price_to_med_xy', 'price_to_med_year', 'outlier_y_3', 'outlier_y_5',\n",
    "                 'outlier_y_10', 'outlier_y_20', 'outlier_xy_2', 'outlier_xy_4',\n",
    "                 'outlier_xy_10', \n",
    "                 'Avg_HHsize', 'Avg_HHinc', 'Avg_own_payt',\n",
    "                 'Avg_val_dwel', 'Avg_rent', 'Pop_x', 'PopDens', 'Dwel', 'DwelDens',\n",
    "                 'Sgl_det', 'Apt_5plus', 'Sgl_att', 'Owned', 'Rented', 'CarTrVan_d',\n",
    "                 'CarTrVan_p', 'PT', 'Walk', 'Bike', 'Lbrfrc', 'Emp', 'Unemp',\n",
    "                 'Not_lbrfrc', 'Employee', 'Self_emp', 'At_home', 'No_fix_wkpl',\n",
    "                 'Usl_wkpl', 'Blue_cljob', 'White_cljob',\n",
    "                 'Pop_y', 'FT_wrk', 'Stu', 'HH', 'Jobs', 'Cars',\n",
    "                 'lucr_detached', 'lucr_duplex_townhouse', 'lucr', 'lucr_condo', 'lucr_other']\n",
    "s = s[all_feat_cols]\n",
    "s_val1 = s_val1[all_feat_cols]\n",
    "s_val2 = s_val2[all_feat_cols]\n",
    "s = s.dropna()\n",
    "s_val1 = s_val1.dropna()\n",
    "s_val2 = s_val2.dropna()\n",
    "print(\"{0:,} rows in train-test subset, {1:,} rows in validation subset #1, {2:,} rows in validation subset #2\"\n",
    "      .format(len(s), len(s_val1), len(s_val2)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Encode target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "le = LabelEncoder()\n",
    "\n",
    "s['lucr_e'] = le.fit_transform(s['lucr'])\n",
    "s_val1['lucr_e'] = le.transform(s_val1['lucr'])\n",
    "s_val2['lucr_e'] = le.transform(s_val2['lucr'])\n",
    "\n",
    "s['lucr_detached_e'] = le.fit_transform(s['lucr_detached'])\n",
    "s_val1['lucr_detached_e'] = le.transform(s_val1['lucr_detached'])\n",
    "s_val2['lucr_detached_e'] = le.transform(s_val2['lucr_detached'])\n",
    "\n",
    "s['lucr_duplex_townhouse_e'] = le.fit_transform(s['lucr_duplex_townhouse'])\n",
    "s_val1['lucr_duplex_townhouse_e'] = le.transform(s_val1['lucr_duplex_townhouse'])\n",
    "s_val2['lucr_duplex_townhouse_e'] = le.transform(s_val2['lucr_duplex_townhouse'])\n",
    "\n",
    "s['lucr_condo_e'] = le.fit_transform(s['lucr_condo'])\n",
    "s_val1['lucr_condo_e'] = le.transform(s_val1['lucr_condo'])\n",
    "s_val2['lucr_condo_e'] = le.transform(s_val2['lucr_condo'])\n",
    "\n",
    "s['lucr_other_e'] = le.fit_transform(s['lucr_other'])\n",
    "s_val1['lucr_other_e'] = le.transform(s_val1['lucr_other'])\n",
    "s_val2['lucr_other_e'] = le.transform(s_val2['lucr_other'])\n",
    "\n",
    "print(s['lucr'].value_counts().sort_index())\n",
    "print(s['lucr_e'].value_counts().sort_index())\n",
    "print(s_val1['lucr'].value_counts().sort_index())\n",
    "print(s_val1['lucr_e'].value_counts().sort_index())\n",
    "print(s_val2['lucr'].value_counts().sort_index())\n",
    "print(s_val2['lucr_e'].value_counts().sort_index())\n",
    "\n",
    "s = s.drop(['lucr', 'lucr_detached', 'lucr_duplex_townhouse', 'lucr_condo', 'lucr_other'], axis=1)\n",
    "s_val1 = s_val1.drop(['lucr', 'lucr_detached', 'lucr_duplex_townhouse', 'lucr_condo', 'lucr_other'], axis=1)\n",
    "s_val2 = s_val2.drop(['lucr', 'lucr_detached', 'lucr_duplex_townhouse', 'lucr_condo', 'lucr_other'], axis=1)\n",
    "print(\"Land use encoded!\")\n",
    "labels_list = ['condo', 'detached', 'duplex_townhouse', 'other']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exploratory Data Analysis (EDA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "condo_corr = s.corr()['lucr_condo_e'].reset_index().rename(columns={'index': 'var', 'variable': 'class'})\n",
    "detached_corr = s.corr()['lucr_detached_e'].reset_index().rename(columns={'index': 'var', 'variable': 'class'})\n",
    "duplex_townhouse_corr = \\\n",
    "    s.corr()['lucr_duplex_townhouse_e'].reset_index().rename(columns={'index': 'var', 'variable': 'class'})\n",
    "other_corr = s.corr()['lucr_other_e'].reset_index().rename(columns={'index': 'var', 'variable': 'class'})\n",
    "\n",
    "targets_corr = pd.merge(\n",
    "    pd.merge(\n",
    "        pd.merge(condo_corr, detached_corr, on='var'), \n",
    "    duplex_townhouse_corr, on='var'), \n",
    "               other_corr, on='var')\n",
    "mask1 = targets_corr['var'].isin(['lucr_e', 'lucr_detached_e', 'lucr_duplex_townhouse_e', 'lucr_condo_e', 'lucr_other_e'])\n",
    "targets_corr = targets_corr[~mask1]\n",
    "targets_corr_tidy = pd.melt(targets_corr, id_vars='var').sort_values('var')\n",
    "\n",
    "print_top = 10\n",
    "print(\"----- Pearson correlation coefficient between features and target classes\"\n",
    "      \"\\n\\n         strongest negative correlation (top {0}):\\n\".format(print_top),\n",
    "      targets_corr_tidy.sort_values('value').head(print_top),\n",
    "      \"\\n\\n         strongest positive correlation (top {0}):\\n\".format(print_top),\n",
    "      targets_corr_tidy.sort_values('value', ascending=False).head(print_top))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f, ax = plt.subplots(1, figsize=(10, 20))\n",
    "sns.barplot(x=\"value\", y=\"var\", hue=\"variable\", data=targets_corr_tidy,\n",
    "            palette=\"muted\", ax=ax)\n",
    "ax.set_ylabel(\"Features\", fontsize=16)\n",
    "ax.set_xlabel(\"Correlation coefficient\", fontsize=16)\n",
    "ax.set_title(\"Pearson correlation coefficient between features and target classes\", fontsize=16)\n",
    "ax.grid(True)\n",
    "ax.legend(loc='center right', fontsize=14)\n",
    "plt.xticks(fontsize=14)\n",
    "plt.show()\n",
    "#plt.savefig(\"../../results/plots/all_features_lucr_corr.png\", dpi=300, bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Select features and target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_cols = 5\n",
    "feat_cols = s.columns[:-target_cols]\n",
    "feat_cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = s[feat_cols].values\n",
    "X_val1 = s_val1[feat_cols].values\n",
    "X_val2 = s_val2[feat_cols].values\n",
    "y = s['lucr_e'].values\n",
    "y_val1 = s_val1['lucr_e'].values\n",
    "y_val2 = s_val2['lucr_e'].values\n",
    "print(\"Features and target selected.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Perform train-test split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=1, stratify=y)\n",
    "print(\"Performed train-test split.\")\n",
    "print('Labels counts in y:', np.bincount(y))\n",
    "print('Labels counts in y_train:', np.bincount(y_train))\n",
    "print('Labels counts in y_test:', np.bincount(y_test))\n",
    "print('Labels counts in y_val1:', np.bincount(y_val1))\n",
    "print('Labels counts in y_val2:', np.bincount(y_val2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Standardize features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc = StandardScaler()\n",
    "sc.fit(X_train)\n",
    "X_train_std = sc.transform(X_train)\n",
    "X_test_std = sc.transform(X_test)\n",
    "X_val1_std = sc.transform(X_val1)\n",
    "X_val2_std = sc.transform(X_val2)\n",
    "print(\"Input features were standardized.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Normalize features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc = MinMaxScaler()\n",
    "sc.fit(X_train)\n",
    "X_train_norm = sc.transform(X_train)\n",
    "X_test_norm = sc.transform(X_test)\n",
    "X_val1_norm = sc.transform(X_val1)\n",
    "X_val2_norm = sc.transform(X_val2)\n",
    "print(\"Input features were normalized.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear Discriminant Analysis (LDA)\n",
    "Linear Discriminant Analysis (LDA) can be used as a technique for feature extraction to increase the computational efficiency and reduce the degree of overfitting due to the curse of dimensionality in non-regularized models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train and test algorithms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lda = LDA(n_components=2)\n",
    "X_train_lda = lda.fit_transform(X_train_std, y_train)\n",
    "X_test_lda = lda.transform(X_test_std)\n",
    "X_val1_lda = lda.transform(X_val1_std)\n",
    "X_val2_lda = lda.transform(X_val2_std)\n",
    "\n",
    "\n",
    "colors = ['r', 'b', 'g', 'k']\n",
    "markers = ['s', 'x', 'o', 'v']\n",
    "for l, c, m in zip(np.unique(y_train), colors, markers):\n",
    "    plt.scatter(X_train_lda[y_train==l, 0], \n",
    "                X_train_lda[y_train==l, 1], \n",
    "                c=c, label=l, marker=m, alpha=0.005) \n",
    "plt.xlabel('PC 1')\n",
    "plt.ylabel('PC 2')\n",
    "plt.legend(loc='lower left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = 'Perceptron (LDA, 2 linear discriminants)'\n",
    "ppn = Perceptron(max_iter=100, eta0=0.01, random_state=1)\n",
    "fit_model(ppn, model_name, X_train_lda, y_train, X_test_lda, y_test, X_val1_lda, y_val1, X_val2_lda, y_val2, \n",
    "          plot_dec_reg='train-test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = 'Logistic regression (LDA, 2 linear discriminants)'\n",
    "lr = LogisticRegression(C=100.0, random_state=1, solver='lbfgs', multi_class='auto')\n",
    "fit_model(lr, model_name, X_train_lda, y_train, X_test_lda, y_test, X_val1_lda, y_val1, X_val2_lda, y_val2, \n",
    "          plot_dec_reg='train-test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "model_name = 'Decision Tree (LDA, 2 linear discriminants)'\n",
    "tree = DecisionTreeClassifier(criterion='gini', max_depth=4, random_state=1)\n",
    "fit_model(tree, model_name, X_train_lda, y_train, X_test_lda, y_test, X_val1_lda, y_val1, X_val2_lda, y_val2, \n",
    "          plot_dec_reg='train-test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = 'Random Forest (LDA, 2 linear discriminants)'\n",
    "forest = RandomForestClassifier(criterion='gini', n_estimators=25, random_state=1, n_jobs=8)\n",
    "fit_model(forest, model_name, X_train_lda, y_train, X_test_lda, y_test, X_val1_lda, y_val1, X_val2_lda, y_val2, \n",
    "          plot_dec_reg='train-test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = 'K-nearest neighbors (LDA, 2 linear discriminants)'\n",
    "knn = KNeighborsClassifier(n_neighbors=5, p=2, metric='minkowski')\n",
    "fit_model(knn, model_name, X_train_lda, y_train, X_test_lda, y_test, X_val1_lda, y_val1, X_val2_lda, y_val2, \n",
    "          plot_dec_reg='train-test')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
